---
title             : "Getting started with Latent Profile Analysis (LPA) in the psychological sciences: A tutorial using the tidyLPA R package that provides an interface to open-source and commercial software"
shorttitle        : "Title"

author: 
  - name          : "Joshua Rosenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "1122 Volunteer Blvd., Knoxville, TN, 37996"
    email         : "jmrosenberg@utk.edu"
  - name          : "Caspar van Lissa"
    affiliation   : "2"
  - name          : "Jennifer Schmidt"
    affiliation   : "3"
  - name          : "Patrick Beymer"
    affiliation   : "3"
  - name          : "Daniel Anderson"
    affiliation   : "4"
  - name          : "Matthew Schell"
    affiliation   : "3"
  - name          : "Rebecca Steingut"
    affiliation   : "5"
  - name          : "Stephanie Wormington"
    affiliation   : "6"

affiliation:
  - id            : "1"
    institution   : "University of Tennessee, Knoxville"
  - id            : "2"
    institution   : "Utrecht University"
  - id            : "3"
    institution   : "Michigan State University"
  - id            : "4"
    institution   : "University of Oregon"
  - id            : "5"
    institution   : "Columbia University"
  - id            : "6"
    institution   : "University of Virginia"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  
keywords          : "Latent Profile Analysis, mixture models, finite mixture models, tutorial, R, MPlus, mclust"
wordcount         : "4,882 (need to shorten to ~ 3,000)" 

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- tidyLPA Tutorial for AMPPS -->
<!-- https://www.psychologicalscience.org/publications/ampps/ampps-submission-guidelines#NEMP -->
<!-- Tutorials -->
<!-- Tutorials provide hands-on, practical guidance for researchers. Any topic that could enhance research practices or methods might be suitable for a Tutorial, provided that the material covered in the Tutorial would be relevant to and useful for the journal’s broad readership; Tutorials of more narrow interest or of relevance to only one subfield or literature typically are not appropriate for AMPPS. 

AMPPS welcomes Tutorials that focus on helping researchers learn to use statistical tools, improve their statistical practices and intuitions, better their data-management and lab practices, enhance the reliability and reproducibility of their research, engage in transparent and open practices, and so on. Tutorials often include dynamic, interactive content and should provide concrete guidance rather than solely abstract principles. Some Tutorials will be solicited by the editorial team, and the team welcomes suggestions and proposals for Tutorials. -->
<!-- Most Tutorials should be brief (< 3,000 words), but they may be longer if necessary to explain the content fully and make it accessible to and usable by readers. -->
<!-- The introduction to a Tutorial typically should be no more than one to two paragraphs long (< 500 words) and should not include an extensive literature review. The introduction should explain the motivation for the Tutorial and highlight how learning the contents will benefit readers. -->
<!-- Tutorials should have a brief summary of their contents, rather than a General Discussion section. -->
<!-- Tutorials should be accompanied by publicly available code and all resources necessary for researchers (and reviewers) to follow the text. -->
<!-- Tutorials can include a list of additional resources (e.g., citations and links) for readers who would like to learn more. -->

# Introduction



# Background

Person-oriented analysis involves a statistical approach to exploring psychological constructs in a way that captures and simplifies complexity. This approach can be used to consider the way in which psychological constructs, for example, are experienced together and at once. Thus, conceptually, this approach emphasizes how individuals go about interacting, socializing, and behaving in this way, experiencing these activities as a whole person. Person-oriented analysis, developed within developmental science, focuses on how individuals have experiences in a holistic way and also how these ways can be identified through common groups, or profiles.(Bergman & El-Khouri, 1997; Magnusson & Cairns, 1996). One way to distinguish the approach is to consider it in contrast to variable-centered approaches, such as those for which the covariability between variables, rather than the relations between profiles of individuals, is the focus (Bergman & Trost, 2006). 

Though studies examining learning from a person-oriented perspective are not very common, some examples include studies of intrinsic and extrinsic motivation (Corpus & Wormington, 2014; Hayenga & Corpus, 2010), profiles of achievement goals (see Wormington & Linnenbrink-Garcia, advance online publication, for a review), and epistemic cognition (Trevors, Kendeou, Braten, & Braasch, 2017). Recently, too, there have been excellent reviews of person-oriented analyses in motivation and educational psychology (Linnenbrink-Garcia & Wormington, 2017). 

In this article, our goal is to review person-oriented analysis focused on getting started (on both knowing about and doing person-oriented analyses) in an informed way. More specifically, we start with an overview of the methodology, describe specific software tools, explain analytic choices made in the course of their use, and conclude with general recommendations One challenge facing those trying to learn about and apply person-oriented analysis in their research is the multitude of terms used for similar analytic approaches or ideas. Throughout this manuscript, we aim to point these out and explain when and why a variety of terms have been used. Thus, this manuscript is written for an informed and curious reader at any level interested in understanding this particular approach.

# Latent Profile Analysis

LPA is a type of finite mixture model (Harring and Hodis, 2017). The goal of this approach is estimate the parameters for a number of distributions (often multivariate) from a single dataset. The approach can be thought of as seeking an answer to the question, Do the data in a sample come from more than one population? Thus, such an approach is model-based, and some descriptions in the literature refer to it as model-based clustering (Hennig, Meila, Murtagh, & Rocci, 2015; Scrucca, Fop, Murphy, & Raftery, 2017). Thus, one distinction between LPA and cluster analytic approaches is that LPA is model-based; instead of using algorithms to group together cases, LPA seeks to estimate parameters - in terms of variances and covariances and how they are the same or different across profiles - that best characterize the different distributions. Then, this approach seeks to assign to each observation a probability that the observation is a sample from the population associated with each profile (or mixture component). 

Because LPA is model-based, a number of different model parameterizations can be estimated. These models differ in terms of whether--and how--parameters are estimated across the profiles. These parameters are the means for the different profiles, which, in this approach, always are estimated freely across the profiles; the variances for the variables used to create the profiles, which can be estimated freely or can be estimated to be the same, or equal, across profiles; and the covariances of the variables used to create the profiles, which can be freely-estimated, estimated to be equal, or fixed to be zero. One challenge facing the analyst using LPA is that these parameters and the distinct model parameterizations that can be estimated is the different terminology used. As one example, Scrucca et al. (2017) refer to these parameterizations not in terms of whether and how parameters are estimated, but rather in terms of the geometric properties of the distributions that result from particular parameterizations. Muthen and Muthen (1997-2017) and others (Pastor et al., 2007) commonly refer to local independence to mean that the covariances are fixed to zero (also described as the specification of the covariance matrix as “diagonal,” because only the diagonal components, or the variances, are estimated). More information on the model parameterizations are discussed in the context of the software tool tidyLPA that we have developed. 

 This section outlines several LPA models and guidelines/examples for when they might be most appropriate in person-centered analysis. In general, as more parameters are estimated (i.e., those that are fixed to zero are estimated as being equal across profiles; or those estimated as being equal across profiles are freely-estimated across them), the model becomes more complex; the model may fit better, but also be overfit, meaning that the profiles identified may be challenging to replicate with another, separate data set. Even still, flexibility in terms of which models can be estimated also has affordances. For example, the varying means, equal variances, and covariances fixed to 0. A researcher might choose this model specification if she wants to model the variables to be used to create profiles that are independent. This model is very simple, as no covariances are estimated and the variances are estimated to be the same across profiles.  As we estimate more parameters (and decrease the degrees of freedom), we are more likely to fit the data, but less likely to be able to replicate the model with a second set of data.  In other words, more parameters may mean a loss of external validity. As we progress toward more complex models (with increasingly complex parameterization, i.e. models 3 and 4 above), then we are more likely to fit the data better.

A number of notes regarding person-oriented analyses are important to raise at the same time as their promise. Bauer (2007) notes that many samples of data can be usefully broken down into profiles, and that the addition of profiles will likely be suggested for reasons other than the samples coming from more than one distribution (i.e., due to non-normality in the variables measured). Bauer also cautions that profiles should not be reified; that profiles do not necessarily exist outside of the analysis that they should be interpreted more as useful interpretative devices. These cautions suggest that, in general, parsimony, interpretability, and a general sense that the profiles are not necessarily real, but are rather helpful analytic tools, should be both priorities for the analyst and the reader of studies using this approach.

# Specific Software Tools 

SPSS is a common tool to carry out cluster analyses. SPSS provides functionality to carry out both hierarchical and k-means cluster analysis. While somewhat straightforward to carry out, particularly in SPSS’s graphical user interface (GUI), there are some challenges to use of this approach. The GUI in SPSS can be challenging, even for the most able analyst, to be able to document every step with syntax, and so reproducing the entire analysis efficiently can be a challenge, both for the analyst exploring various solutions and for the reviewer looking to replicate this work. Additionally, SPSS is commercial software (and is expensive), and so analysts without the software cannot carry out this analysis.                                                                         

Accordingly, we worked to develop tools, initially used by the research team and now shared more widely, to provide basic functionality to carry out Latent Profile Analysis (LPA). In addition to being freely-available and open-source (i.e., anyone can inspect the source code), the tools described lend themselves to an open science. It is especially important to have an open transparent methodology when using person-centered approaches because of the high dependence on researcher judgment at multiple points in the process.

Accordingly, we set out to develop packages that a) provided sensible defaults and were easy to use, but provided the option to access and modify all of the inputs to the model (i.e., low barrier, high ceiling), b) interfaced to existing tools,, and are able to translate between what existing tools are capable of and what researchers and analysts carrying-out person-oriented analyses would like to specify, c) made it easy to carry-out fully-reproducible analyses and d) were well-documented. Both of these packages--one for two-step cluster analysis and one for LPA--are designed to do one thing well, namely, quickly and easily estimate profiles using cluster analysis or a model-based approach (Latent Profile Analysis). 

One way that tidyLPA is designed to be easy to use is that it assumes a “tidy” data structure (Wickham, 2014). This means that it emphasizes the use of a data frame as both the primary input and output of functions for the package. Because data is passed to and returned (in amended form, i.e., with the latent profile probabilities and classes appended to the data) from the function, it makes it easy to create plots or use results in subsequent analyses. Another noteworthy feature of tidyLPA is that it provides the same functionality through two different tools, one that is open-source and available through R, the mclust package (Scrucca et al., 2017) and one that is available through the commercial software MPlus (Muthen & Muthen, 1997-2017). Moreover, as both tools use the same maximum likelihood estimation procedure, they are benchmarked to produce the same output (see here). Also, note that we have described the model specifications with descriptions of what is estimated in terms of the variances and covariances, the common names for the models (i.e., class-varying unrestricted), and the covariance matrix associated with the parameterizationfor the six models that are possible to be estimated on the website for tidyLPA (see here). 

# Analytic Choices

There are a number of analytic choices that need to be made when carrying out person-oriented analyses. Because such person-oriented approaches are often more subjective (in practice) than other approaches (Linnenbrink-Garcia and Wormington, 2017), there is no one rule for determining the solution obtained. This solution is obtained on the basis of multiple decisions, such as the number of profiles selected or the modeling decisions such as what specific options are used for the cluster analysis (i.e., the distance metric used to calculate the similarity of the observations as part of the Ward’s hierarchical clustering) or what parameters are estimated and how as part of LPA.

Given the subjectivity involved, it is important that researchers be transparent and work as part of a team to obtain clustering solutions. Transparency about the design and analytic choices is important so that readers can appropriately interpret the report. Researchers can enhance transparency and reproducibility by sharing detailed descriptions of methodology and document it through the use of syntax (and, if possible, data) that we share with others. Working as part of a team can help to serve as a check on several of the choices researchers make, such as over-fitting or under-fitting the model to the data. Each decision depends on multiple factors and balancing tensions. We discuss each of the key decisions listed in an analysis.

## How the Data are Centering or Scaled

The data can be transformed by centering or scaling. Typically, these are done after the profiles are created, so that differences between profiles can be more easily explored. They can also be done prior to the analysis, which can be helpful for obtaining solutions when the variables are on very different scales..

## How to Choose the Number of Profiles

In the case of choosing the number of profiles (and the specification of the model / profile solution), multiple criteria, such as the BIC or the proportion of variance explained are recommended for decision-making, but also interpretability in light of theory, parsimony, and evidence from cross-validation should be considered. For cluster analysis, the proportion of variance explained can be helpful for selecting a number of candidate profiles. Then, the analyst should focus on the interpretability of the solution as well as concerns of parsimony; if two profile solutions are similar, then the more simple solution may be preferred. 

## How to Select the Model Parameterization

This applies most to LPA, but in cluster analysis, there are also options about the distance metric used. In LPA, we can determine which parameters - variance and covariance - are estimated to be the same or different across all profiles, and, in the case of covariance, whether it is fixed to zero. Of course, for the profiles we are most interested in, the mean is allowed to vary across profiles. The models with more parameters freely-estimated use more degrees of freedom; thus, similar to the number of profiles selected, the balance between adding more parameters (and having a more complex model) and interpretability and parsimony must be balanced. If a more complex model fits better and is interpretable and is justifiable in terms of the data collected, then it may be preferred to simpler models. If the two models are similar in terms of their fit, then the more simple, parsimonious model should be selected. 

# More General Recommendations

This section highlights three general recommendations. First, scholars taking a person-oriented approach should emphasize reproducibility in carrying out analyses. This is in part due to the exploratory and interpretative nature of person-oriented approaches. To this end, presenting multiple models, as in Pastor et al. (2007), should be encouraged, rather than presenting one solution. In addition, having multiple analysts review the solutions found is encouraged. As part of this recommendation, we suggest that researchers consider how their analyses can be reproduced by other analysts outside of the research team: Sharing code and data is an important part of this work. Also related, researchers should consider how profiles are replicated across samples. 

A second general recommendation considers more flexibly incorporating time, when data are collected across multiple time points, into analyses. ISOA groups all time points and doesn’t make distinctions. Other approaches perform analysis separately at different timepoints (Corpus & Wormington, 2014). Some integrate time as a part of the profiles, i.e. growth mixture modeling (groups of patterns), i.e. within-person growth modeling, where there are individual growth patterns. Research to date has yet to consider additional challenges in applying person centered approaches to longitudinal data. For instance, Schmidt et al (2018) use an Experience Sampling Method (ESM) approach to collecting data and used a person-centered approach to generate profiles of students in science class. This work does not account for student-level effects. In other words, it did not model the shared variance of multiple observations of the same student.

Third, best practices in within-person or longitudinal research call for modeling the nesting structure. However, researchers have yet to successfully incorporate this practice into the person centered approach. One way to approach this is to use cross-classified mixed effects models, as in Strati, Schmidt, and Maier (2017) and in Rosenberg (2018). In such approaches, dependencies in terms of, for example, individuals responding to (ESM) surveys at the same time and repeated responses being associated with the same individuals can both be modeled, although the effects of these two sources of dependencies are not nested as in very common uses of multi-level models, but rather are cross-classified. West, Welch, & Galecki (2014) have a description of the use of multi-level models with cross-classified data and tools (including those freely available through R) that can be used to estimate them.

# Rationale

Latent Profile Analysis (LPA) is a statistical modeling approach for estimating distinct profiles of variables. In the social sciences and in educational research, these profiles could represent, for example, how different youth experience dimensions of being engaged (i.e., cognitively, behaviorally, and affectively) at the same time. Note that LPA works best with continuous variables (and, in some cases, ordinal variables), but is not appropriate for dichotomous (binary) variables.

Many analysts have carried out LPA using a latent variable modeling approach. From this approach, different parameters - means, variances, and covariances - are freely estimated across profiles, fixed to be the same across profiles, or constrained to be zero. The MPlus software is commonly used to estimate these models (see [here](https://www.statmodel.com/examples/mixture.shtml)) using the expectation-maximization (EM) algorithm to obtain the maximum likelihood estimates for the parameters. 

Different *models* (or how or whether parameters are estimated) can be specified and estimated. While MPlus is widely-used (and powerful), it is costly, closed-source, and can be difficult to use, particularly with respect to interpreting or using the output of specified models as part of a reproducible workflow. 


# Description of the goals of tidyLPA

The goal of tidyLPA is to make it easy to carry out LPA using R. In particular, tidyLPA provides an interface to the powerful and widely-used [mclust](https://www.stat.washington.edu/mclust/) package for Gaussian Mixture Modeling. This means that tidyLPA does not contain code to carry out LPA directly, but rather provides "wrappers" to mclust functions that make them easier to use. The primary contributions of tidyLPA are to:

1. Provide functionality to specify models that are common to LPA
2. Make it easier to use the output in subsequent analysis through a ["tidy" interface](https://CRAN.R-project.org/package=tidyverse/vignettes/manifesto.html), in that:
- input and output are both a `data.frame` (specifically its modified version, a `tibble`) that can be used to create plots or can be used in subsequent analyses
- uses the "pipe" operator, `%>%` to compose functions
- being designed and documented to be easy to use, especially for beginners (but also to provide options for finer-grained choices for estimating the model and for viewing more specific forms of the LPA output)

## Software approach to carrying out LPA: Interface to mclust (and to MPlus)

In the open-source R software, there is not yet a tool to easily carry out LPA, though there are many tools that one could use to. For example, the [R version of OpenMx](https://openmx.ssri.psu.edu/) can be used for this purpose (and to specify almost any model possible to specify within a latent variable modeling approach). However, while OpenMx is very flexible, it can also be challenging to use. 

Other tools in R allow for estimating Gaussian mixture models, or models of multivariate Gaussian (or normal) distributions. In this framework, the term "mixture component" has a similar meaning to a profile. While much more constraining than the latent variable modeling framework, the approach is often similar or the same: the EM algorithm is used to (aim to) obtain the maximum likelihood estimates for the parameters being estimated. Like in the latent variable modeling framework, different models can be specified. 

In addition to following the same general approach, using tools that are designed for Gaussian mixture modeling have other benefits, some efficiency-related (see [RMixMod](https://cran.r-project.org/package=Rmixmod), which uses compiled C++ code) and others in terms of ease-of-use (i.e., the plot methods built-in to RMixMod, mclust, and other tools). However, they also have some drawbacks, in that it can be difficult to translate between the model specifications, which are often described in terms of the geometric properties of the multivariate distributions being estimated (i.e., "spherical, equal volume"), rather than in terms of whether and how the means, variances, and covariances are estimated. They also may use different default settings (than those encountered in MPlus) in terms of the expectation-maximization algorithm, which can make comparing results across tools challenging.

This package focuses on models that are commonly specified as part of LPA. Because MPlus is so widely-used, it can be helpful to compare output from other software to MPlus. The functions in tidyLPA that use mclust have been benchmarked to MPlus for a series of simple models (with small datasets and for models with small numbers of profiles. This [R Markdown output](https://jrosen48.github.io/r-markdown/comparing-mplus-mclust.html) contains information on how mclust and Mplus compare. The R Markdown to generate the output is also available [here](https://jrosen48.github.io/r-markdown/comparing-mplus-mclust.Rmd), and, as long as you have purchased MPlus (and installed MplusAutomation), can be used to replicate all of the results for the benchmark. Note that most of the output is identical, thoughthere are some differences in the hundreths decimal places for some. Because of differences in settings for the EM algorithm and particularly for the start values (random starts for MPlus and starting values from hierarchical clustering for mclust), differences may be expected for more complex data and models. An important direction for the development of tidyLPA (the functions that use mclust) is to continue to understand when and why the output differs from MPlus output. Note that tidyLPA also provides functions to interface to MPlus, though these are not the focus of the package, as they require MPlus to be purchased and installed in order to be used. 

# tidyLPA use

tidyLPA aims to make Latent Profile Analysis (LPA) even more accessible to a broad audience, by means of a user-friendly interface, and to fit more seemlessly into a 'tidyverse' workflow.

As before, **tidyLPA** offers a parallel workflow for the open-source package Mclust, and the commercial package *Mplus*.

These changes necessitated some simplification of the existing functions. Advanced options are still available, but require slightly more work on the user's part. The upside of this is that, for most users, the tidyLPA workflow and documentation are substantially simplified.

# Example

## Installation

You can install tidyLPA from CRAN with:

```{r, eval = FALSE}
install.packages("tidyLPA")
```

You can also install the development version of tidyLPA from GitHub with:

```{r gh-installation, eval = FALSE}
install.packages("devtools")
devtools::install_github("data-edu/tidyLPA")
```

## Mclust

Here is a brief example using the built-in `pisaUSA15` data set and variables for broad interest, enjoyment, and self-efficacy. Note that we first type the name of the data frame, followed by the unquoted names of the variables used to create the profiles. We also specify the number of profiles and the model. See `?estimate_profiles` for more details.

In these examples, we pass the results of one function to the next by *piping* (using the `%>%` operator, loaded from the `dplyr` package). We pass the data to a function that selects relevant variables, and then to `estimate_profiles`:

```{r, message = F}
library(tidyLPA)
library(tidyverse)
```

```{r}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    single_imputation() %>%
    estimate_profiles(3)
```

## Mplus

We can use Mplus simply by changing the package argument for `estimate_profiles()` (not run):

```{r, eval = FALSE}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    single_imputation() %>%
    estimate_profiles(3, package = "MplusAutomation")
```

A simple summary of the analysis is printed to the console (and its posterior probability). The resulting object can be further passed down a pipeline to other functions, such as `plot`, `compare_solutions`, `get_data`, `get_fit`, etc. This is the "tidy" part, in that the function can be embedded in a tidy analysis pipeline.

If you have Mplus installed, you can call the version of this function that uses MPlus in the same way, by adding the argument `package = "MplusAutomation`.

We can plot the profiles by piping the output to `plot_profiles()`:

```{r, eval = TRUE}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    single_imputation() %>%
    scale() %>%
    estimate_profiles(3) %>% 
    plot_profiles()
```

## Comparing a wide range of solutions

The function `compare_solutions()` compares the fit of several estimated models, with varying numbers of profiles and model specifications:

```{r, eval = TRUE}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    single_imputation() %>%
    estimate_profiles(1:3, 
                      variances = c("equal", "varying"),
                      covariances = c("zero", "varying")) %>%
    compare_solutions(statistics = c("AIC", "BIC"))
```


## Model specification

In addition to the number of profiles (specified with the `n_profiles` argument), the model can be specified in terms of whether and how the variable variances and covariances are estimated.

The models are specified by passing arguments to the `variance` and `covariance` arguments. The possible values for these arguments are:

- `variances`: "equal" and "zero"
- `covariances`: "varying", "equal", and "zero"

If no values are specified for these, then the variances are constrained to be equal across classes, and covariances are fixed to 0 (conditional independence of the indicators).

These arguments allow for four models to be specified:

* Equal variances and covariances fixed to 0 (Model 1)
* Varying variances and covariances fixed to 0 (Model 2)
* Equal variances and equal covariances (Model 3)
* Varying variances and varying covariances (Model 6)

Two additional models (Models 4 and 5) can be fit using MPlus. More information on the models can be found in the [vignette](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html).

Here is an example of specifying a model with varying variances and covariances (Model 6):

```{r, eval = TRUE}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    estimate_profiles(3, 
                      variances = "varying",
                      covariances = "varying")
```

In general, the approach to choosing the model is similar to choosing the number of profiles, requiring **deciding on the basis of evidence from multiple sources**, including information criteria, statistical tests, and concerns of interpretability and parsimony. The article by [Pastor and colleagues (2007)](http://www.sciencedirect.com/science/article/pii/S0361476X06000543) has helpful information on the model specifications. Here, the six models that are possible to specify in LPA are described in terms of how the variables used to create the profiles are estimated. 

Note that *p* represents different profiles and each parameterization is represented by a 4 x 4 covariance matrix and therefore would represent the parameterization for a four-profile solution. In all of the models, the means are estimated freely in the different profiles. Imagine that each row and column represents a different variable, i.e., the first row (and column) represents broad interest, the second enjoyment, the third self-efficacy, and the fourth another variable, i.e., future goals and plans.

### 1. Equal variances, and covariances fixed to 0 (model 1)

In this model, which corresponds to the mclust model wit the name "EEI", the variances are estimated to be equal across profiles, indicated by the absence of a p subscript for any of the diagonal elements of the matrix. The covariances are constrained to be zero, as indicated by the 0's between every combination of the variables. 

It is specified with `variances = "equal"` and `covariances = "zero"`. 

This model is highly constrained but also parsimonious: the profiles are estimated in such a way that the variables' variances are identical for each of the profiles, and the relationships between the variables are not estimated. In this way, less degrees of freedom are taken used to explain the observations that make up the data. However, estimating more parameters--as in the other models--may better explain the data, justifying the addition in complexity that their addition involves (and their reduction in degrees of freedom). This model is sometimes referred to as a *class-invariant* parameterization.

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & 0 & 0 & 0 \\ 0 & { \sigma  }_{ 2 }^{ 2 } & 0 & 0 \\ 0 & 0 & { \sigma  }_{ 3 }^{ 2 } & 0 \\ 0 & 0 & 0 & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] 
$$

### 2. Varying variances and covariances fixed to 0 (model 2)

This model corresponds to the mclust model "VVI" and allows for the variances to be freely estimated across profiles. The covariances are constrained to zero. 

It is specified with `variances = "varying"` and `covariances = "zero"`. 

Thus, it is more flexible (and less parsimonious) than model 1, but in terms of the covariances, is more constrained than model 2. This model is sometimes referred to as a *class-varying diagonal* parameterization.

$$ 
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & 0 & 0 & 0 \\ 0 & { \sigma  }_{ 2p }^{ 2 } & 0 & 0 \\ 0 & 0 & { \sigma  }_{ 3p }^{ 2 } & 0 \\ 0 & 0 & 0 & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$


### 3. Equal variances and equal covariances (model 3)

This model corresponds to the mclust model "EEE". In this model, the variances are still constrained to be the same across the profiles, although now the covariances are estimated (but like the variances, are constrained to be the same across profiles). 

It is specified with `variances = "equal"` and `covariances = "equal"`. 

Thus, this model is the first to estimate the covariance (or correlations) of the variables used to create the profiles, thus adding more information that can be used to better understand the characteristics of the profiles (and, potentially, better explain the data). This model is sometimes referred to as a *class-invariant unrestricted* parameterization.

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & { \sigma  }_{ 21 } & { \sigma  }_{ 31 } & { \sigma  }_{ 41 } \\ { \sigma  }_{ 12 } & { \sigma  }_{ 2 }^{ 2 } & { \sigma  }_{ 23 } & { \sigma  }_{ 24 } \\ { \sigma  }_{ 13 } & { \sigma  }_{ 12 } & { \sigma  }_{ 3 }^{ 2 } & { \sigma  }_{ 33 } \\ { \sigma  }_{ 14 } & { \sigma  }_{ 12 } & { \sigma  }_{ 12 } & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] 
$$


### 4. Varying means, varying variances, and equal covariances (model 4)

This model, which specifies for the variances to be freely estimated across the profiles and for the covariances to be estimated to be equal across profiles, extends model 3. 

It is specified with `variances = "varying"` and `covariances = "equal"`. 

Unfortunately, this model cannot be specified with mclust, though it can be with MPlus; this model *can* be used with the functions to interface to MPlus described below.

$$
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & { \sigma  }_{ 21 } & { \sigma  }_{ 31 } & { \sigma  }_{ 41 } \\ { \sigma  }_{ 12 } & { \sigma  }_{ 2p }^{ 2 } & { \sigma  }_{ 23 } & { \sigma  }_{ 24 } \\ { \sigma  }_{ 13 } & { \sigma  }_{ 12 } & { \sigma  }_{ 3p }^{ 2 } & { \sigma  }_{ 33 } \\ { \sigma  }_{ 14 } & { \sigma  }_{ 12 } & { \sigma  }_{ 12 } & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$

### 5. Varying means, equal variances, and varying covariances (model 5)

This model specifies the variances to be equal across the profiles, but allows the covariances to be freely estimated across the profiles. 

It is specified with `variances = "equal"` and `covariances = "varying"`. 

Like model 4, this model cannot be specified with mclust, though it can be with MPlus. Again, this model *can* be used with the functions to interface to MPlus described below.

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & { \sigma  }_{ 21p } & { \sigma  }_{ 31p } & { \sigma  }_{ 41p } \\ { \sigma  }_{ 12p } & { \sigma  }_{ 2 }^{ 2 } & { \sigma  }_{ 23p } & { \sigma  }_{ 24p } \\ { \sigma  }_{ 13p } & { \sigma  }_{ 12p } & { \sigma  }_{ 3 }^{ 2 } & { \sigma  }_{ 33p } \\ { \sigma  }_{ 14p } & { \sigma  }_{ 12p } & { \sigma  }_{ 12p } & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] \quad 
$$

### 6. Varying variances and varying covariances (model 4)

This model corresponds to the mclust model "VVV". It allows the variances and the covariances to be freely estimated across profiles. 

It is specified with `variances = "varying"` and `covariances = "varying"`. 

Thus, it is the most complex model, with the potential to allow for understanding many aspects of the variables that are used to estimate the profiles and how they are related. However, it is less parsimonious than all of the other models, and the added parameters should be considered in light of how preferred this model is relative to those with more simple specifications. This model is sometimes referred to as a *class-varying unrestricted* parameterization.

$$
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & { \sigma  }_{ 21p } & { \sigma  }_{ 31p } & { \sigma  }_{ 41p } \\ { \sigma  }_{ 12p } & { \sigma  }_{ 2p }^{ 2 } & { \sigma  }_{ 23p } & { \sigma  }_{ 24p } \\ { \sigma  }_{ 13p } & { \sigma  }_{ 12p } & { \sigma  }_{ 3p }^{ 2 } & { \sigma  }_{ 33p } \\ { \sigma  }_{ 14p } & { \sigma  }_{ 12p } & { \sigma  }_{ 12p } & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$

# Other options

There is a lot of output that is possible to obtain from the `estimate_profiles()` function - much more than a tidy data frame, which is the default. The easiest way to access it is by using the `get_estimates()` function.

```{r, eval = TRUE}
m3 <- pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    estimate_profiles(3)

get_estimates(m3)
```

Other options include how the raw data is processed. 

We can center or scale the data before estimating the profiles with the `scale()` or `poms()` functions:

```{r, eval = TRUE}
pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    scale() %>%
    estimate_profiles(4) %>%
    plot_profiles()

pisaUSA15[1:100, ] %>%
    select(broad_interest, enjoyment, self_efficacy) %>%
    poms() %>%
    estimate_profiles(4) %>%
    plot_profiles()
```

Since we often wish to use the estimated profiles in subsequent analyses, we may want the original `data.frame`, with variables that are predictors or outcomes of the profiles, included. Here, we created profiles with just two of the three variables, to demonstrate how the third variable is still returned in the output. We can return this `data.frame`, and not just one with the variables used to create the profiles and the profile assignments (and posterior probabilities), using the function `get_data()`

```{r, eval = TRUE}
get_data(m3)
```
# Conclusion

Person-oriented analysis ia way to consider how psychological constructs are experienced (and can be analyzed) together and at once. Though described in contrast to a variable-centered approach, scholars have pointed out how person-oriented approaches are complementary to variable-centered analyses (Marsh, Ludtke, Trautwein, & Morin, 2009). A person-oriented approach can help us to consider multiple variables together and at once and in a more dynamic way, reflected in the methodological approaches for cluster analysis and LPA that identify profiles of individuals responses.

This manuscript provided an outline of how to get started with person-oriented analyses in an informed way. We provided a general overview of the methodology and described tools to carry out such an analysis. We also described specific tools, emphasizing freely-available open-source options that we have developed. Because of the inherently exploratory nature of person-oriented analysis, carrying out the analysis in a trustworthy and open way is particularly important. In this way, the interpretative aspect of settling on a solution shares some features of quantitative and qualitative research: The systematic nature of quantitative research methods (focused upon agreed-upon criteria such as likelihood-ratio tests) and qualitative research methods (focused upon the trustworthiness of both the analysis and the analyst) are important to consider when carrying out person-oriented analysis. Lastly, we made some general recommendations for future directions--and also highlighted some situations for which person-oriented approaches may not be the best and some cautions raised in past research regarding how such approaches are used. 

In conclusion, as use of person-oriented approaches expand, new questions and opportunities for carrying out research in a more holistic, dynamic way will be presented. Analyzing constructs together and at once is appealing to researchers, particularly those carrying out research in fields such as education for which communicating findings to stakeholders in a way that has the chance to impact practice is important. Our aim was not to suggest that such an approach is always the goal or should always be carried out, but rather to describe how researchers may get started in an informed way as researchers seek to understand how individuals interact, behave, and learn in ways that embraces the complexity of these experiences.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```
Bergman, L. R., & Magnusson, D. (1997). A person-oriented approach in research on developmental psychopathology. Development and psychopathology, 9(2), 291-319.  
Bergman, L. R., & Trost, K. (2006). The person-oriented versus the variable-oriented approach: Are they complementary, opposites, or exploring different worlds?. Merrill-Palmer Quarterly, 52(3), 601-632.
Collins, L. M., & Lanza, S. T. (2010). Latent class and latent transition analysis: With applications in the social, behavioral, and health sciences (Vol. 718). John Wiley & Sons.
Corpus, J. H., & Wormington, S. V. (2014). Profiles of intrinsic and extrinsic motivations in elementary school: A longitudinal analysis. The Journal of Experimental Education, 82(4), 480-501.
Corpus, J. H., & Wormington, S. V. (2014). Profiles of intrinsic and extrinsic motivations in elementary school: A longitudinal analysis. The Journal of Experimental Education, 82(4), 480-501.
Harring, J. R., & Hodis, F. A. (2016). Mixture modeling: Applications in educational psychology. Educational Psychologist, 51(3-4), 354-367.
Hayenga, A. O., & Corpus, J. H. (2010). Profiles of intrinsic and extrinsic motivations: A person-centered approach to motivation and achievement in middle school. Motivation and Emotion, 34(4), 371-383.
Linnenbrink-Garcia, L., & Wormington, S. V. (2017). Key challenges and potential solutions for studying the complexity of motivation in schooling: An integrative, dynamic person-oriented perspective. British Journal of Educational Psychology monograph series II. Psychological aspects of education: Current trends—the role of competence beliefs in teaching and learning. Chichester, UK: Wiley.
Magnusson, D., & Cairns, R. B. (1996). Developmental science: Toward a unified framework. Cambridge, England: Cambridge University Press.  
Marsh, H. W., Lüdtke, O., Trautwein, U., & Morin, A. J. (2009). Classical latent profile analysis of academic self-concept dimensions: Synergy of person-and variable-centered approaches to theoretical models of self-concept. Structural Equation Modeling, 16(2), 191-225.
McLachlan, G. J. (2011). Commentary on Steinley and Brusco (2011): Recommendations and cautions.
Muthen, L. K., & Muthen, B. O. (1997-2017). Mplus User’s Guide. Los Angeles, CA: Muthén & Muthén.
Rosenberg, J. M., Schmidt, J. A., Beymer, P. N., & Steingut, R. R. (2018). Interface to mclust to easily carry out Latent Profile Analysis [Statistical software for R]. https://github.com/jrosen48/tidyLPA
Rosenberg, J. M., Schmidt, J. A., Beymer, P. N., & Steingut, R. R. (2017). prcr: Person-Centered Analysis. R package version 0.1.5. https://CRAN.R-project.org/package=prcr
Scrucca L., Fop M., Murphy T. B. and Raftery A. E. (2017) mclust 5: clustering, classification and density estimation using Gaussian finite mixture models The R Journal 8/1, pp. 205-233
Hennig, C., Meila, M., Murtagh, F., & Rocci, R. (Eds.). (2015). Handbook of cluster analysis. CRC Press.
Steinley, D., & Brusco, M. J. (2011). Evaluating mixture modeling for clustering: Recommendations and cautions. Psychological Methods, 16(1), 63.
Steinley, D., & Brusco, M. J. (2011). K-means clustering and mixture model clustering: Reply to McLachlan (2011) and Vermunt (2011).
Trevors, G. J., Kendeou, P., Bråten, I., & Braasch, J. L. (2017). Adolescents’ epistemic profiles in the service of knowledge revision. Contemporary Educational Psychology, 49, 107-120.
Vermunt, J. K. (2011). K-means may perform as well as mixture model clustering but may also be much worse: Comment on Steinley and Brusco (2011).
West, B. T., Welch, K. B., & Galecki, A. T. (2014). Linear mixed models: a practical guide using statistical software. Chapman and Hall/CRC.
Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1-23.

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
